{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Predicting tweet sentiments</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You work in an event management company. On Mother's Day, your company has organized an event where they want to cast positive Mother's Day related tweets in a presentation. Data engineers have already collected the data related to Mother's Day that must be categorized into positive, negative, and neutral tweets.\n",
    "\n",
    "You are appointed as a Machine Learning Engineer for this project. Your task is to build a model that helps the company classify these sentiments of the tweets into positive, negative, and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set consists of six columns:\n",
    "\n",
    "Column Name\tDescription\n",
    "- id\tID of tweet\n",
    "- original_text\tText of tweet\n",
    "- lang\tLanguage of tweet\n",
    "- retweet_count\tNumber of times retweeted\n",
    "- original_author\tTwitter handle of Author\n",
    "- sentiment_class\tSentiment of Tweet (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn packages\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, xgboost, numpy, string #textblob,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# nltk packages\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "# import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "# vaderSentiment\n",
    "# good - 0.34\n",
    "# bad - -.45\n",
    "# better - 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Let’s get a feel for our data. Here are the first 5 rows of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.245025e+18  Happy #MothersDay to all you amazing mothers o...   en   \n",
       "1  1.245759e+18  Happy Mothers Day Mum - I'm sorry I can't be t...   en   \n",
       "2  1.246087e+18  Happy mothers day To all This doing a mothers ...   en   \n",
       "3  1.244803e+18  Happy mothers day to this beautiful woman...ro...   en   \n",
       "4  1.244876e+18  Remembering the 3 most amazing ladies who made...   en   \n",
       "\n",
       "  retweet_count  original_author  sentiment_class  \n",
       "0             0      BeenXXPired                0  \n",
       "1             1   FestiveFeeling                0  \n",
       "2             0     KrisAllenSak               -1  \n",
       "3             0       Queenuchee                0  \n",
       "4             0  brittan17446794               -1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.246628e+18</td>\n",
       "      <td>3. Yeah, I once cooked potatoes when I was 3 y...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245898e+18</td>\n",
       "      <td>Happy Mother's Day to all the mums, step-mums,...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>iiarushii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244717e+18</td>\n",
       "      <td>I love the people from the UK, however, when I...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245730e+18</td>\n",
       "      <td>Happy 81st Birthday Happy Mother’s Day to my m...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>TheBookTweeters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244636e+18</td>\n",
       "      <td>Happy Mothers day to all those wonderful mothe...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.246628e+18  3. Yeah, I once cooked potatoes when I was 3 y...   en   \n",
       "1  1.245898e+18  Happy Mother's Day to all the mums, step-mums,...   en   \n",
       "2  1.244717e+18  I love the people from the UK, however, when I...   en   \n",
       "3  1.245730e+18  Happy 81st Birthday Happy Mother’s Day to my m...   en   \n",
       "4  1.244636e+18  Happy Mothers day to all those wonderful mothe...   en   \n",
       "\n",
       "  retweet_count  original_author  \n",
       "0             0        LToddWood  \n",
       "1             0        iiarushii  \n",
       "2             0   andreaanderegg  \n",
       "3             1  TheBookTweeters  \n",
       "4             0   andreaanderegg  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1701\n",
       "-1     769\n",
       " 1     765\n",
       "Name: sentiment_class, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\va4nfjfq5b\", \" \")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['original_text']=train['original_text'].apply(tweet_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy mothersday to all you amazing mothers out there i know it s hard not being able to see your mothers today but it s on all of us to do what we can to protect the most vulnerable members of our society beatcoronavirus pic twitter com va nfjfq b'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['original_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['original_text']=test['original_text'].apply(tweet_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah i once cooked potatoes when i was years old and by that i mean i threw a bag of spuds into the toilet happy mothers day i made breakfast that time i thought i was cool by drawing the naked lady dachshund on the overhead projector in psychology class'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['original_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining stopwords: using the one that comes with nltk + appending it with words seen from the above evaluation\n",
    "stop_words = stopwords.words('english')\n",
    "stop_append = ['.', ',', '`', '\"', \"'\", '!', ';', 'nfjfq ', 'com', '%', 'twitter ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "We have an immense number of word forms as you can see from our various counts in the `FreqDist` above - it is helpful for many applications to normalize these word forms (especially applications like search) into some canonical word for further exploration. In English (and many other languages) - morphological context indicate gender, tense, quantity, etc. but these "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .2 POS tagging and Lemma tokenization\n",
    "In the description text a word might appear in different forms while actually representing the same word, like good and best. To reduce noise we try to find the basic form (lemma) of each word in the text. We will rely on the nltk package to achieve this, but first we have to type tag each word, wether they are nouns, verbs, adjectives or adverbs. For that we built up some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list of word types (nouns and adjectives) to leave in the text\n",
    "# defTags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJS', 'JJR']#, 'RB', 'RBS', 'RBR', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "# # functions to determine the type of a word\n",
    "# def is_noun(tag):\n",
    "#     return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "# def is_verb(tag):\n",
    "#     return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "# def is_adverb(tag):\n",
    "#     return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "# def is_adjective(tag):\n",
    "#     return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "# # transform tag forms\n",
    "# def penn_to_wn(tag):\n",
    "#     if is_adjective(tag):\n",
    "#         return nltk.stem.wordnet.wordnet.ADJ\n",
    "#     elif is_noun(tag):\n",
    "#         return nltk.stem.wordnet.wordnet.NOUN\n",
    "#     elif is_adverb(tag):\n",
    "#         return nltk.stem.wordnet.wordnet.ADV\n",
    "#     elif is_verb(tag):\n",
    "#         return nltk.stem.wordnet.wordnet.VERB\n",
    "#     return nltk.stem.wordnet.wordnet.NOUN\n",
    "    \n",
    "# # lemmatizer + tokenizer (+ stemming) class\n",
    "# class LemmaTokenizer(object):\n",
    "#     def __init__(self):\n",
    "#         self.wnl = WordNetLemmatizer()\n",
    "#         # we define (but not use) a stemming method, uncomment the last line in __call__ to get stemming tooo\n",
    "#         self.stemmer = nltk.stem.SnowballStemmer('english') \n",
    "#     def __call__(self, doc):\n",
    "#         # pattern for numbers | words of length=2 | punctuations | words of length=1\n",
    "#         pattern = re.compile(r'[0-9]+|\\b[\\w]{2,2}\\b|[%.,_`!\"&?\\')({~@;:#}+-]+|\\b[\\w]{1,1}\\b')\n",
    "#         # tokenize document\n",
    "#         doc_tok = word_tokenize(doc)\n",
    "#         #filter out patterns from words\n",
    "#         doc_tok = [x for x in doc_tok if x not in stop_words1]\n",
    "#         doc_tok = [pattern.sub('', x) for x in doc_tok]\n",
    "#         # get rid of anything with length=1\n",
    "#         doc_tok = [x for x in doc_tok if len(x) > 1]\n",
    "#         # position tagging\n",
    "#         doc_tagged = nltk.pos_tag(doc_tok)\n",
    "#         # selecting nouns and adjectives\n",
    "#         doc_tagged = [(t[0], t[1]) for t in doc_tagged if t[1] in defTags]\n",
    "#         # preparing lemmatization\n",
    "#         doc = [(t[0], penn_to_wn(t[1])) for t in doc_tagged]\n",
    "#         # lemmatization\n",
    "#         doc = [self.wnl.lemmatize(t[0], t[1]) for t in doc]\n",
    "#         # uncomment if you want stemming as well\n",
    "#         #doc = [self.stemmer.stem(x) for x in doc]\n",
    "#         return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a dataframe using texts and lables\n",
    "\n",
    "texts=train['original_text']\n",
    "labels=train['sentiment_class']\n",
    "#  split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = train_test_split(texts, labels,test_size=0.3, random_state = 42)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorizer\n",
    "To transform the text based description column into a number based object, we use term frequency-inverse document frequency (tf-idf) vectorization. First it counts the occurance of a word in one sample and than down-weighs it with the occurance of the same word over all the samples. It repeats this process for each word and outputs a vector of numbers, where each number represents a word. The actual vectorizer performes some firther normalization too. We have decided to analyse the text word by word (1-grams), but you could choose 2-grams or n-grams, meaning selecting the first n words to represent the first element of the vector. Then each consequent element would be constructed by shifting the n-range by one. The vectorizer accepts a tokenizer as an input, that is why we have defined one previously. The vectorizer gives a lot of opportunities for different setups and tests. If you want to see a smaller experiment with it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(texts)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "\n",
    "# transform the validation data using count vectorizer object\n",
    "testx=test['original_text']\n",
    "xtest_count =  count_vect.transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:501: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(texts)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(texts)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)\n",
    "\n",
    "zvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test['original_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "#     add_element(d,(metrics.accuracy_score(predictions, valid_y)),accuracy_score)\n",
    "#     add_element(d,,f1_score)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "        \n",
    "    return (f1_score(predictions, valid_y,average='weighted') * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Baye's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.99      0.51      0.67       956\n",
      "           2       0.00      0.10      0.01        10\n",
      "\n",
      "    accuracy                           0.50       971\n",
      "   macro avg       0.33      0.20      0.23       971\n",
      "weighted avg       0.97      0.50      0.66       971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_nb=MultinomialNB()\n",
    "classifier_nb.fit( xtrain_tfidf_ngram, train_y)\n",
    "predictions = classifier_nb.predict(xvalid_tfidf_ngram)\n",
    "acc=metrics.accuracy_score(predictions, valid_y)\n",
    "score=f1_score(predictions, valid_y,average='weighted') * 100\n",
    "print(classification_report(predictions, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = {'criterion': ['entropy', 'gini'],\n",
    "               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n",
    "               'max_features': ['auto', 'sqrt','log2', None],\n",
    "               'min_samples_leaf': [4, 6, 8, 12],\n",
    "               'min_samples_split': [5, 7, 10, 14],\n",
    "               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:  5.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_iter=8,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [10, 142, 274, 406, 538,\n",
       "                                                      671, 803, 935, 1067, 1200,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt', 'log2',\n",
       "                                                         None],\n",
       "                                        'min_samples_leaf': [4, 6, 8, 12],\n",
       "                                        'min_samples_split': [5, 7, 10, 14],\n",
       "                                        'n_estimators': [151, 267, 384, 500,\n",
       "                                                         617, 733, 850, 966,\n",
       "                                                         1083, 1200]},\n",
       "                   random_state=101, verbose=5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "model = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 8, \n",
    "                               cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\n",
    "model.fit(xtrain_count,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x3372b1e908>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "table = pd.pivot_table(pd.DataFrame(model.cv_results_),\n",
    "    values='mean_test_score', index='param_n_estimators', \n",
    "                       columns='param_criterion')\n",
    "     \n",
    "sns.heatmap(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(xvalid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.16826265389876"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pred, valid_y,average='weighted') * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       246\n",
      "           1     0.5057    1.0000    0.6717       491\n",
      "           2     0.0000    0.0000    0.0000       234\n",
      "\n",
      "    accuracy                         0.5057       971\n",
      "   macro avg     0.1686    0.3333    0.2239       971\n",
      "weighted avg     0.2557    0.5057    0.3396       971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_y, model.predict(xvalid_count), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.57%\n",
      "\n",
      "F1 Score: 67.17\n",
      "\n",
      "COnfusion Matrix:\n",
      " [[  0   0   0]\n",
      " [246 491 234]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(pred,valid_y) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(pred,valid_y,average='weighted') * 100))\n",
    "print(\"\\nCOnfusion Matrix:\\n\", confusion_matrix(pred,valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  61.26796478275662\n",
      "NB, WordLevel TF-IDF:  67.14589164078278\n",
      "NB, N-Gram Vectors:  65.87204511748638\n",
      "NB, CharLevel Vectors:  67.16826265389876\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_models(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_models(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_models(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_models(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  45.274529669617245\n",
      "LR, WordLevel TF-IDF:  61.152896432797576\n",
      "LR, N-Gram Vectors:  61.29481961549862\n",
      "LR, CharLevel Vectors:  63.88332450152644\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_models(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_models(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_models(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_models(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  66.12837700302121\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_models(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  66.2489563832848\n",
      "RF, WordLevel TF-IDF:  65.17927722687078\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_models(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_models(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  64.29899633222536\n",
      "Xgb, WordLevel TF-IDF:  63.92860551951701\n",
      "Xgb, CharLevel Vectors:  63.62441713365571\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_models(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_models(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_models(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.49948506694129763\n",
      "score 65.87204511748638\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy',acc)\n",
    "print('score',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_fin = classifier_nb.predict(zvalid_tfidf_ngram_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(list(prediction_fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=encoder.inverse_transform(prediction_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment_class']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.246628e+18</td>\n",
       "      <td>3. Yeah, I once cooked potatoes when I was 3 y...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245898e+18</td>\n",
       "      <td>Happy Mother's Day to all the mums, step-mums,...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>iiarushii</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244717e+18</td>\n",
       "      <td>I love the people from the UK, however, when I...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245730e+18</td>\n",
       "      <td>Happy 81st Birthday Happy Mother’s Day to my m...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>TheBookTweeters</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244636e+18</td>\n",
       "      <td>Happy Mothers day to all those wonderful mothe...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.246628e+18  3. Yeah, I once cooked potatoes when I was 3 y...   en   \n",
       "1  1.245898e+18  Happy Mother's Day to all the mums, step-mums,...   en   \n",
       "2  1.244717e+18  I love the people from the UK, however, when I...   en   \n",
       "3  1.245730e+18  Happy 81st Birthday Happy Mother’s Day to my m...   en   \n",
       "4  1.244636e+18  Happy Mothers day to all those wonderful mothe...   en   \n",
       "\n",
       "  retweet_count  original_author  sentiment_class  \n",
       "0             0        LToddWood                0  \n",
       "1             0        iiarushii                0  \n",
       "2             0   andreaanderegg                0  \n",
       "3             1  TheBookTweeters                0  \n",
       "4             0   andreaanderegg                0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.246628e+18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245898e+18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.244717e+18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245730e+18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244636e+18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  sentiment_class\n",
       "0  1.246628e+18                0\n",
       "1  1.245898e+18                0\n",
       "2  1.244717e+18                0\n",
       "3  1.245730e+18                0\n",
       "4  1.244636e+18                0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=pd.DataFrame(test[['id','sentiment_class']])\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('submission_file.csv',index=False,sep=',')\n",
    "# dt.to_csv('file_name.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost classifier\n",
    "XGBoost is a version of gradient boosted decision tree classifier. In boosting, the trees are built sequentially such that each subsequent tree aims to reduce the errors of the previous tree. These subsequent trees are called base or weak learners. Each of these weak learners contributes some vital information for prediction, enabling the boosting technique to produce a strong learner by effectively combining these weak learners. The power of XGBoost lies in its scalability, which drives fast learning through parallel and distributed computing and offers efficient memory usage.\n",
    "\n",
    "We will not do a very detailed hyperparameter optimization becuase it is very time consuming, but rather predifine certain input arguments that control the performance of the classifier and use one argument to otpimize. subsample controls the ratio of the randomly selected training samples before growing the tree. It ranges between 0 and 1. Higher values tend to cause overfitting. colsample_bytree denotes the fraction of columns to be randomly sampled for each tree. n_estimators controls the number of trees to be constructed during the classification process. We will optimize that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.original_text\n",
    "y = train.sentiment_class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.505664263645726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       246\n",
      "           0       0.51      1.00      0.67       491\n",
      "           1       0.00      0.00      0.00       234\n",
      "\n",
      "    accuracy                           0.51       971\n",
      "   macro avg       0.17      0.33      0.22       971\n",
      "weighted avg       0.26      0.51      0.34       971\n",
      "\n",
      "0.3396459007524644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# %%time\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % metrics.accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5036045314109165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       246\n",
      "           0       0.50      1.00      0.67       491\n",
      "           1       0.00      0.00      0.00       234\n",
      "\n",
      "    accuracy                           0.50       971\n",
      "   macro avg       0.17      0.33      0.22       971\n",
      "weighted avg       0.26      0.50      0.34       971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# %%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % metrics.accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = train['original_text']\n",
    "df_y = train['sentiment_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def Tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, learning_rate=0.02, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(clean_text(row).split(), [index]) for index, row in enumerate(df_x)]\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skl_utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(clean_text(row).split())\n",
    "                                     for index, row in enumerate(df_x)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3235/3235 [00:00<00:00, 305385.28it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 64714.26it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 403970.87it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 645876.50it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 149336.59it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 461720.27it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 301215.94it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 419236.01it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 404006.95it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 461688.84it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 807749.34it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 807989.84it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 403946.81it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 646461.17it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 646368.78it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 403958.84it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 404043.04it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 539012.97it/s]\n",
      "100%|██████████| 3235/3235 [00:00<?, ?it/s]\n",
      "100%|██████████| 3235/3235 [00:00<00:00, 403994.92it/s]\n"
     ]
    }
   ],
   "source": [
    "doc2vec_trf = Doc2VecTransformer()\n",
    "doc2vec_features = doc2vec_trf.fit(df_x).transform(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 [00:00<00:00, 165690.15it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369401.35it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431004.91it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517119.66it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646545.88it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430953.58it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862112.52it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430850.95it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430850.95it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646276.42it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431073.38it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646699.96it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861907.16it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646237.94it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431313.18it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165788.84it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 280813.83it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 198890.72it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 645584.56it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646353.39it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861291.66it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369439.07it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430953.58it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369300.81it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431022.03it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862317.98it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517440.12it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861907.16it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 187072.10it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517095.02it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861701.89it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646237.94it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647896.55it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430953.58it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 387604.31it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431004.91it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 648903.56it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 643517.83it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 650653.88it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430868.05it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430936.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 645584.56it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 863003.56it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430850.95it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369401.35it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 172375.96it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 648477.13it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646237.94it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369363.64it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 645738.18it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862112.52it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430885.15it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430987.80it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431039.14it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369539.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430987.80it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369388.78it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517218.22it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647008.33it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862044.06it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431022.03it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430885.15it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430936.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430850.95it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517144.29it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517144.29it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862934.95it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646699.96it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 343421.25it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 580256.52it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862044.06it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 442766.31it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 235055.41it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 642527.45it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517193.57it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 268757.80it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430987.80it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165576.42it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861701.89it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 393234.99it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369250.56it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165632.00it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 107734.12it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369363.64it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 251432.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for XGBoost Classifier :  0.5180834621329211\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "pl_xgb = Pipeline(steps=[('doc2vec',Doc2VecTransformer()),\n",
    "                         ('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])\n",
    "scores = cross_val_score(pl_xgb, df_x, df_y, cv=5)\n",
    "print('Accuracy for XGBoost Classifier : ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Text2TfIdfTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = TfidfVectorizer()\n",
    "        pass\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        df_x = df_x.apply(lambda x : clean_text(x))\n",
    "        self._model.fit(df_x)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return self._model.transform(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = Text2TfIdfTransformer()\n",
    "tfidf_vectors = tfidf_transformer.fit(df_x).transform(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3235, 12442)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\kjk\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Tf-Idf & Logistic Regression:  0.5137557959814527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pl_log_reg_tf_idf = Pipeline(steps=[('tfidf',Text2TfIdfTransformer()),\n",
    "                             ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=100))])\n",
    "scores = cross_val_score(pl_log_reg_tf_idf, df_x, df_y, cv=5,scoring='accuracy')\n",
    "print('Accuracy for Tf-Idf & Logistic Regression: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 [00:00<00:00, 862181.00it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646314.90it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646430.37it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646391.87it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646430.37it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 868666.67it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369388.78it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 519644.73it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431193.25it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431022.03it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430816.75it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431862.29it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 860881.81it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 287256.77it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431141.87it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646430.37it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165546.11it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165629.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646353.39it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369376.21it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647046.90it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431107.62it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517193.57it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 235065.59it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646122.54it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 184199.20it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517242.86it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517045.76it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 876947.71it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430987.80it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861633.49it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 141078.46it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 359658.68it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369388.78it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862317.98it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430936.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430885.15it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 287256.77it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165639.58it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369401.35it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517119.66it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517144.29it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517045.76it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 161607.59it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517095.02it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861975.60it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430868.05it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862112.52it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 126416.26it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431022.03it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517095.02it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 860745.28it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 127969.19it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430731.27it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517366.13it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 648554.62it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 649019.96it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 323321.08it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647008.33it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 172365.01it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430936.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431039.14it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646507.37it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517144.29it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517218.22it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646738.49it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 652531.33it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 1292706.77it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 516971.89it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369388.78it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862386.49it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 305856.83it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517588.15it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431022.03it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430868.05it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646777.02it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430987.80it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647124.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RandomForest :  0.5180834621329212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pl_random_forest = Pipeline(steps=[('doc2vec',Doc2VecTransformer()),\n",
    "                                   ('random_forest', RandomForestClassifier())])\n",
    "scores = cross_val_score(pl_random_forest, df_x, df_y, cv=5,scoring='accuracy')\n",
    "print('Accuracy for RandomForest : ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 393805.64it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165596.62it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430953.58it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430902.26it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369376.21it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646815.56it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646391.87it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 143645.49it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430885.15it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 637396.29it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165589.05it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369351.08it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 648748.43it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369401.35it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861975.60it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 860950.09it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369376.21it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 338094.40it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431022.03it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430953.58it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369451.64it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 258578.31it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 1292091.27it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646391.87it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646237.94it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 516996.51it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646199.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862592.08it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517489.45it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369300.81it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369413.92it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646314.90it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369363.64it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517168.93it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646353.39it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 323176.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430902.26it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646237.94it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861086.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647317.00it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431022.03it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 863209.44it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517045.76it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 378125.85it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517242.86it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517168.93it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646199.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165637.05it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517095.02it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647162.63it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 188760.46it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369451.64it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646045.63it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 861838.73it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430868.05it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 165616.84it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369351.08it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647201.21it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646391.87it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 287271.97it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 287325.20it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430526.27it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517144.29it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369338.51it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 369376.21it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646276.42it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647278.40it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 647085.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 643784.99it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 215476.79it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431039.14it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430970.69it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646353.39it/s]\n",
      "100%|██████████| 2588/2588 [00:00<?, ?it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 384814.90it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 323224.81it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646199.47it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430868.05it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 431004.91it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 430919.36it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 862044.06it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646430.37it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517119.66it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 517070.39it/s]\n",
      "100%|██████████| 2588/2588 [00:00<00:00, 646661.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for XGBoost Classifier :  0.5069551777434311\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "pl_xgb = Pipeline(steps=[('doc2vec',Doc2VecTransformer()),\n",
    "                         ('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])\n",
    "scores = cross_val_score(pl_xgb, df_x, df_y, cv=5)\n",
    "print('Accuracy for XGBoost Classifier : ', scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
